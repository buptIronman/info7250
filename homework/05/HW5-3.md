# Secondary Sorting

## Requirements

Re do HW3-Part3, but use SecondarySorting to sort the values based on AccessDate in a Descending Order.

## Analysis

The data format in the 'access.log' files is as below:

`129.10.135.165 - - [15/Oct/2011:11:59:10 -0400] "GET / HTTP/1.1" 200 6`

**The keys to implement secondary sorting are:**

* Composite key: Define the key pair as a Java object implementing interface `WritableComparable`.
* Grouping Comparator: Defines how keys will be grouped together.
* Partitioner: Define which reducer the output of a map will go.

The first step is to implement the secondary sorting to sort the access date based on the IP addresses, then  count the total times of an IP address.

Note: Use secondary sorting won't affect the counting of the occurance of an IP address. That's why I use the first step to verify the secondary sorting use another MapReduce job.

## Composite key

```java
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.util.Date;
import org.apache.hadoop.io.WritableComparable;

public class IPDatePair implements WritableComparable <IPDatePair> {
    private String IP;
    private long timestamp; // transform date to timestamp for easy serialization
    
    IPDatePair(){
        this.IP = "";
        timestamp = 0;
    }
    
    IPDatePair(String IP, Date date) {
        this.setIP(IP);
        this.setDate(date);
    }

    String getIP() {
        return IP;
    }

    void setIP(String IP) {
        this.IP = IP;
    }

    long getDate() {
        return timestamp;
    }
    
    void setDate(Date date) {
        this.timestamp = date.getTime();
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        this.IP = in.readUTF();
        this.timestamp = in.readLong();
    }

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeUTF(IP);
        out.writeLong(timestamp);
    }

    @Override
    public int compareTo(IPDatePair other) {
        int compareValue = this.getIP().compareTo(other.getIP());
        if (compareValue == 0) {
            long compareDate = this.getDate() - other.getDate();
            if (compareDate == 0) compareValue = 0;
            else compareValue = compareDate < 0? -1:1;
        }
        
        return -1*compareValue;  // sort descending
    }
    
    @Override
    public String toString() {
        return this.IP + "\t" + new Date(this.timestamp).toString();
    }
}
```

## Grouping Comparator

```java
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;


public class IPDateGroupingComparator extends WritableComparator {

     public IPDateGroupingComparator() {
         super(IPDatePair.class, true);
     }

     @Override
     /**
      * This comparator controls which keys are grouped 
      * together into a single call to the reduce() method
      */
     public int compare(WritableComparable wc1, WritableComparable wc2) {
         IPDatePair pair = (IPDatePair) wc1;
         IPDatePair other = (IPDatePair) wc2;
         return pair.getIP().compareTo(other.getIP());
     }
 }
```

## Partitioner

```java
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;

public class IPDatePartitioner extends Partitioner<IPDatePair, Text> {
    @Override
    public int getPartition(IPDatePair pair, Text text, int numberOfPartitions) {
        // make sure that partitions are non-negative
        return Math.abs(pair.getIP().hashCode() % numberOfPartitions);
    }
}
```

## Mapper

```java
    public final static SimpleDateFormat formatter = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z");

    public static class TheMapper extends Mapper<Object, Text, IPDatePair, IntWritable> {
        private final static IntWritable one = new IntWritable(1);

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            String IP = line.split(" ")[0];
            Date date = formatter.parse(line, new ParsePosition(line.indexOf('[') + 1));

            IPDatePair reducerKey = new IPDatePair(IP, date);
            context.write(reducerKey, one);
        }
    }
```

## Reducer - Verify Secondary Sorting

Straight output every value in the iterable to verify the ording or the access date.

```java
    public static class TheReducer extends Reducer<IPDatePair, IntWritable, IPDatePair, LongWritable> {
        private static LongWritable one = new LongWritable(1);
        public void reduce(IPDatePair key, Iterable<IntWritable> values, Context context)
                throws IOException, InterruptedException {
            for(IntWritable v: values) context.write(key, one);
        }
    }
```

## MapReduce Driver

```java
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.text.ParsePosition;
import java.text.SimpleDateFormat;
import java.util.Date;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Partitioner;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Reducer.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;


/**
 * Using the access.log file stored in HDFS, implement MapReduce to find the
 * number of times each IP accessed the web site. Also, use SecondarySorting to
 * sort the values based on AccessDate in a Descending Order.
 * 
 * access.log 
 * 127.0.0.1 - - [15/Oct/2011:11:49:11 -0400] "GET / HTTP/1.1" 200 44
 * 127.0.0.1 - - [15/Oct/2011:11:49:11 -0400] "GET /favicon.ico HTTP/1.1" 404
 * 129.10.135.165 - - [15/Oct/2011:11:59:10 -0400] "GET / HTTP/1.1" 200 6
 * ...
 * 
 * ref: https://www.safaribooksonline.com/library/view/data-algorithms/9781491906170/ch01.html#mapleft_parenthesisright_parenthesis_for
 * 
 * @author bin
 * @created 2018-04-01
 */

public class IPSecondarySortingMR extends Configured implements Tool {
    
    public final static SimpleDateFormat formatter = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z");

    public static class TheMapper extends Mapper<Object, Text, IPDatePair, IntWritable> {
        private final static IntWritable one = new IntWritable(1);

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            String line = value.toString();
            String IP = line.split(" ")[0];
            Date date = formatter.parse(line, new ParsePosition(line.indexOf('[') + 1));

            IPDatePair reducerKey = new IPDatePair(IP, date);
            context.write(reducerKey, one);
        }
    }    
    
//    This reducer is to verify secondary sorting. 
    public static class TheReducer extends Reducer<IPDatePair, IntWritable, IPDatePair, LongWritable> {
        private static LongWritable one = new LongWritable(1);
        public void reduce(IPDatePair key, Iterable<IntWritable> values, Context context)
                throws IOException, InterruptedException {
            for(IntWritable v: values) context.write(key, one);
        }
    }
    
    @Override
    public int run(String[] args) throws Exception {
        Configuration conf = this.getConf();

        Job job = Job.getInstance(conf, "IP Counter - Secondary Sorting");
        job.setJarByClass(IPSecondarySortingMR.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        job.setInputFormatClass(TextInputFormat.class);

        job.setMapperClass(TheMapper.class);
        job.setMapOutputKeyClass(IPDatePair.class);
        job.setMapOutputValueClass(IntWritable.class);
        job.setPartitionerClass(IPDatePartitioner.class);
        job.setGroupingComparatorClass(IPDateGroupingComparator.class);        
        
        job.setReducerClass(TheReducer.class);
        job.setNumReduceTasks(1); // map only to verify the secondary sorting
        
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        job.setOutputKeyClass(IPDatePair.class);
        job.setOutputValueClass(LongWritable.class);

        return job.waitForCompletion(true) ? 0 : 1;
    }

    public static void main(String[] args) {
        int code = -1;
        try {
            code = ToolRunner.run(new Configuration(), new IPSecondarySortingMR(), args);
        } catch (Exception e) {
            e.printStackTrace();
        }
        System.exit(code);
    }

}
```

## Execution

1. Go to the `bin` in eclipse, and package the 'class' files in a jar
   `jar -cf IPSecondarySortingMR.jar IPSecondarySortingMR*.class IPDate*.class`
2. Upload the jar file to hadoop server/cluster
   `scp IPSecondarySortingMR.jar ubuntu@ip-172-31-25-109.us-west-2.compute.internal:~/downloads/`
3. Update the 'access.log' to HDFS 
   `hadoop fs -put access.log access-log/input `
4. Run the MapReduce job
   `hadoop jar IPSecondarySortingMR.jar IPSecondarySortingMR access-log/input access-log/output_secondary_sorting`

## Running Log

```bash
$hadoop jar IPSecondarySortingMR.jar IPSecondarySortingMR access-log/input access-log/output_secondary_sorting
2018-04-02 14:21:00,131 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
2018-04-02 14:21:00,543 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1522702780500_0004
2018-04-02 14:21:01,153 INFO input.FileInputFormat: Total input files to process : 1
2018-04-02 14:21:01,219 INFO mapreduce.JobSubmitter: number of splits:1
2018-04-02 14:21:01,253 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
2018-04-02 14:21:01,363 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1522702780500_0004
2018-04-02 14:21:01,365 INFO mapreduce.JobSubmitter: Executing with tokens: []
2018-04-02 14:21:01,532 INFO conf.Configuration: resource-types.xml not found
2018-04-02 14:21:01,532 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2018-04-02 14:21:01,594 INFO impl.YarnClientImpl: Submitted application application_1522702780500_0004
2018-04-02 14:21:01,628 INFO mapreduce.Job: The url to track the job: http://ip-172-31-25-109.us-west-2.compute.internal:8088/proxy/application_1522702780500_0004/
2018-04-02 14:21:01,628 INFO mapreduce.Job: Running job: job_1522702780500_0004
2018-04-02 14:21:07,730 INFO mapreduce.Job: Job job_1522702780500_0004 running in uber mode : false
2018-04-02 14:21:07,733 INFO mapreduce.Job:  map 0% reduce 0%
2018-04-02 14:21:13,794 INFO mapreduce.Job:  map 100% reduce 0%
2018-04-02 14:21:18,825 INFO mapreduce.Job:  map 100% reduce 100%
2018-04-02 14:21:19,839 INFO mapreduce.Job: Job job_1522702780500_0004 completed successfully
2018-04-02 14:21:19,919 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=1025512
		FILE: Number of bytes written=2462435
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=3497793
		HDFS: Number of bytes written=1587282
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=7112
		Total time spent by all reduces in occupied slots (ms)=5554
		Total time spent by all map tasks (ms)=3556
		Total time spent by all reduce tasks (ms)=2777
		Total vcore-milliseconds taken by all map tasks=3556
		Total vcore-milliseconds taken by all reduce tasks=2777
		Total megabyte-milliseconds taken by all map tasks=7282688
		Total megabyte-milliseconds taken by all reduce tasks=5687296
	Map-Reduce Framework
		Map input records=35111
		Map output records=35111
		Map output bytes=955284
		Map output materialized bytes=1025512
		Input split bytes=126
		Combine input records=0
		Combine output records=0
		Reduce input groups=1945
		Reduce shuffle bytes=1025512
		Reduce input records=35111
		Reduce output records=35111
		Spilled Records=70222
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=177
		CPU time spent (ms)=3570
		Physical memory (bytes) snapshot=603889664
		Virtual memory (bytes) snapshot=6205411328
		Total committed heap usage (bytes)=417333248
		Peak Map Physical memory (bytes)=377204736
		Peak Map Virtual memory (bytes)=3099574272
		Peak Reduce Physical memory (bytes)=226684928
		Peak Reduce Virtual memory (bytes)=3105837056
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=3497667
	File Output Format Counters 
		Bytes Written=1587282
```

## Verification - Secondary Sorting

Check the output to verify the dates are sorted in desending order for an IP address.

```bash
$ fs -cat access-log/output_secondary_sorting/part-r-00000 | head -n 25
99.15.75.165	Fri Aug 10 07:50:16 PDT 2012	1
98.229.69.153	Mon Apr 01 04:54:58 PDT 2013	1
98.229.69.153	Mon Apr 01 04:54:58 PDT 2013	1
98.229.25.220	Mon Apr 01 14:23:42 PDT 2013	1
98.229.25.220	Sun Mar 31 19:03:48 PDT 2013	1
98.217.196.55	Wed Apr 10 09:09:45 PDT 2013	1
98.217.196.55	Wed Apr 10 09:09:38 PDT 2013	1
98.217.196.55	Wed Apr 10 09:09:38 PDT 2013	1
98.217.196.55	Wed Apr 10 09:09:38 PDT 2013	1
98.217.196.55	Wed Apr 10 08:59:39 PDT 2013	1
98.217.196.55	Wed Apr 10 08:59:39 PDT 2013	1
98.217.196.55	Wed Apr 10 08:58:26 PDT 2013	1
98.217.196.55	Wed Apr 10 08:58:26 PDT 2013	1
98.217.196.55	Wed Apr 10 08:55:39 PDT 2013	1
98.217.196.55	Wed Apr 10 08:55:39 PDT 2013	1
98.217.196.55	Wed Apr 10 08:55:09 PDT 2013	1
98.217.196.55	Wed Apr 10 08:55:08 PDT 2013	1
98.217.196.55	Tue Apr 02 08:44:10 PDT 2013	1
98.217.196.55	Sun Mar 31 21:37:29 PDT 2013	1
98.217.196.55	Sun Mar 31 20:12:36 PDT 2013	1
98.217.124.237	Thu Nov 15 18:21:50 PST 2012	1
98.217.117.57	Sun Apr 29 19:25:19 PDT 2012	1
98.217.117.57	Sun Apr 29 19:23:53 PDT 2012	1
98.217.117.57	Sun Apr 29 19:23:53 PDT 2012	1
98.217.117.57	Sun Apr 29 19:23:53 PDT 2012	1
```

Taking `98.217.196.55` for example, the access dates are in descending order.

## Reducer - Count the Occurance of Each IP

Count is simple, and I also incoude **the last access date ( = the first value in the iterable)** in the output for fun, which means the input of reducer is sorted by the second part (timestamp) of the composite key.

```java
    public static class TheReducer extends Reducer<IPDatePair, IntWritable, IPDatePair, LongWritable> {

        public void reduce(IPDatePair key, Iterable<IntWritable> values, Context context)
                throws IOException, InterruptedException {
            long timestamp = -1;
            long sum = 0;
            
            for (IntWritable val : values) {
                if (timestamp == -1) timestamp = key.getDate();
                sum += val.get();
            }
            
            IPDatePair pair = new IPDatePair(key.getIP(), new Date(timestamp)); 
            LongWritable  outputValue = new LongWritable(sum);
            context.write(pair, outputValue);
        }
    }
```

## Execution

The excution is the same with the previous MapReduce job. 

## Result- IP Counting with the Last Access Date

Here is the first 10 lines of the running result. The full list is attached as `HW5-3.IP-count.txt`.

```json
99.15.75.165	Fri Aug 10 07:50:16 PDT 2012	1
98.229.69.153	Mon Apr 01 04:54:58 PDT 2013	2
98.229.25.220	Mon Apr 01 14:23:42 PDT 2013	2
98.217.196.55	Wed Apr 10 09:09:45 PDT 2013	15
98.217.124.237	Thu Nov 15 18:21:50 PST 2012	1
98.217.117.57	Sun Apr 29 19:25:19 PDT 2012	6
98.216.59.134	Thu Dec 13 07:36:23 PST 2012	3
98.174.140.238	Thu Jan 26 22:00:21 PST 2012	2
98.143.169.213	Thu Sep 20 00:24:04 PDT 2012	3
98.124.178.144	Mon Jan 02 04:21:32 PST 2012	6
```

## Verification - IP Counting

Taking '98.217.196.55'  for example, the total number  is '15' ,and the last access date is '`Wed Apr 10 09:09:45 PDT 2013' as showed in the MapReduce output above.

Here is the count directly in the access.log file.

```bash
$grep 98.217.196.55 access.log | wc -l
15
```

Also, the lastest time is  `10/Apr/2013:12:09:45 -0400` (the last record) as below, which prove the equality. (There is some timezone difference but it is understandable.)

```
$ grep 98.217.196.55 access.log
98.217.196.55 - - [31/Mar/2013:23:12:36 -0400] "GET /yusuf/emgt.jpg HTTP/1.1" 200 233195
98.217.196.55 - - [01/Apr/2013:00:37:29 -0400] "GET /yusuf/emgt.jpg HTTP/1.1" 304 -
98.217.196.55 - - [02/Apr/2013:11:44:10 -0400] "GET /yusuf/emgt.jpg HTTP/1.1" 304 -
98.217.196.55 - - [10/Apr/2013:11:55:08 -0400] "GET /yusuf/tiles HTTP/1.1" 404 209
98.217.196.55 - - [10/Apr/2013:11:55:09 -0400] "GET /favicon.ico HTTP/1.1" 404 209
98.217.196.55 - - [10/Apr/2013:11:55:39 -0400] "GET /yusuf/tiles HTTP/1.1" 404 209
98.217.196.55 - - [10/Apr/2013:11:55:39 -0400] "GET /favicon.ico HTTP/1.1" 404 209
98.217.196.55 - - [10/Apr/2013:11:58:26 -0400] "GET /yusuf/tiles HTTP/1.1" 404 209
98.217.196.55 - - [10/Apr/2013:11:58:26 -0400] "GET /favicon.ico HTTP/1.1" 404 209
98.217.196.55 - - [10/Apr/2013:11:59:39 -0400] "GET /yusuf/tiles HTTP/1.1" 404 209
98.217.196.55 - - [10/Apr/2013:11:59:39 -0400] "GET /favicon.ico HTTP/1.1" 404 209
98.217.196.55 - - [10/Apr/2013:12:09:38 -0400] "GET /yusuf/tiles HTTP/1.1" 301 242
98.217.196.55 - - [10/Apr/2013:12:09:38 -0400] "GET /yusuf/tiles/ HTTP/1.1" 200 304
98.217.196.55 - - [10/Apr/2013:12:09:38 -0400] "GET /favicon.ico HTTP/1.1" 404 209
98.217.196.55 - - [10/Apr/2013:12:09:45 -0400] "GET /yusuf/tiles/SpringTilesVelocity.zip HTTP/1.1" 200 42968801
```

## Further Thinking

* The output of reducer could be sorted using another MapReduce job with using the 'last access date' as the key.
* It is not nessasary to use the 'Secondary Sorting' to get the last access date because sorting is expensive. An alternative way is to use 'IP' as the key, and 'date' as the value in the map. And in the reducer, simple select the max date values and cound the number of date as the occurance of IPs.