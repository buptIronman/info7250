# Requirements

Determine the average `stock_price_adj_close` value by the year.
Choose an implementation in which a Reducer could be used as a Combiner. (discussed in the lecture, and available in the slides).

# Assumption

The average `stock_price_adj_close` value by the year means the average of all the stocks.

#  Analysis

**Average by Year**

The key is `year` and the value is the `stock_price_adj_close`.

**Reducer as Combiner**

There are two ways to do this:

* Use 'count' and running 'sum'. In the end, divide 'sum' by 'count' to get the average. However, the 'sum' might be overflow (even use long) if there are huge amount of data.
* Use 'count' and 'average'. The 'global average' is 'sum of (sub count * sub average)' / 'sum of sub count'. However, this might lead to minor percision missing in the process of dividing and multiplying. 

I will choose the second way because data overflow might lead to ridiculious result (might be negative).

**Value in Map Representation**

Define a costomized writable (tuple) having 'count' and 'average'.

# CountAverageTuple

```java
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

import org.apache.hadoop.io.Writable;

public class CountAverageTuple implements Writable {
    private int count = 0;
    private float average;

    public void readFields(DataInput in) throws IOException {
        count = in.readInt();
        average = in.readFloat();
    }

    public void write(DataOutput out) throws IOException {
        out.writeInt(count);
        out.writeFloat(average);
    }

    public int getCount() {
        return count;
    }

    public void setCount(int count) {
        this.count = count;
    }

    public float getAverage() {
        return average;
    }

    public void setAverage(float average) {
        this.average = average;
    }

    @Override
    public String toString() {
        return count + "\t" + average;
    }

    @Override
    public int hashCode() {
        final int prime = 31;
        int result = 1;
        result = prime * result + Float.floatToIntBits(average);
        result = prime * result + count;
        return result;
    }

    @Override
    public boolean equals(Object obj) {
        if (this == obj)
            return true;
        if (obj == null)
            return false;
        if (getClass() != obj.getClass())
            return false;
        CountAverageTuple other = (CountAverageTuple) obj;
        if (Float.floatToIntBits(average) != Float.floatToIntBits(other.average))
            return false;
        if (count != other.count)
            return false;
        return true;
    }

}
```



# MapReduce

```java
import java.io.IOException;
import java.text.SimpleDateFormat;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

/**
 * Determine the average stock_price_adj_close value by the year.
 * Choose an implementation in which a Reducer could be used as a Combiner.
 *  (discussed in the lecture, and available in the slides).
 * 
 * @author bin
 *
 */
public class StockPriceAverageMR extends Configured implements Tool {
    
    public final static SimpleDateFormat dateFormatter = new SimpleDateFormat("yyyy-MM-dd");

    public static class TheMapper extends Mapper<Object, Text, IntWritable, CountAverageTuple> {
        
        private IntWritable year = new IntWritable();
        private CountAverageTuple tuple = new CountAverageTuple();

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            String[] fields = value.toString().split(",");
            if (fields == null || fields.length != 9) {
                System.err.println("Bad data. The expected format is 9 columns seperated by comma.");
                return; // skip the bad data
            }

            try {
                year.set(dateFormatter.parse(fields[2]).getYear() + 1900);
                float price = Float.parseFloat(fields[8]);
                tuple.setAverage(price);
                tuple.setCount(1);
            } catch (Exception e) {
                System.err.println("Bad data. Cannot parse the symbol, date, volume and price in column 2, 3, 8 and 9.");
                e.printStackTrace();
                return;
            }
            
            context.write(year, tuple);
        }
    }    

    public static class TheReducer extends Reducer<IntWritable, CountAverageTuple, IntWritable, CountAverageTuple> {

        private CountAverageTuple tuple = new CountAverageTuple();
        
        @Override
        public void reduce(IntWritable key, Iterable<CountAverageTuple> values, Context context)
                throws IOException, InterruptedException {
            int count = 0;
            long sum = 0;
            
            for (CountAverageTuple v: values) {
                count += v.getCount(); // total count
                sum += v.getCount() * v.getAverage(); // running sum
            }
            tuple.setCount(count);
            float averagePrice = (float)1.0*sum/count;
            tuple.setAverage(averagePrice);
            context.write(key, tuple);
        }
    }
    
    @Override
    public int run(String[] args) throws Exception {
        Configuration conf = this.getConf();

        Job job = Job.getInstance(conf, "Stock Annual Average");
        job.setJarByClass(StockPriceAverageMR.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        job.setInputFormatClass(TextInputFormat.class);

        job.setMapperClass(TheMapper.class);
        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(CountAverageTuple.class);
        job.setCombinerClass(TheReducer.class);
        
        job.setReducerClass(TheReducer.class);
        job.setNumReduceTasks(1);
        
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        job.setOutputKeyClass(IntWritable.class);
        job.setOutputValueClass(CountAverageTuple.class);

        return job.waitForCompletion(true) ? 0 : 1;
    }

    public static void main(String[] args) {
        int code = -1;
        try {
            code = ToolRunner.run(new Configuration(), new StockPriceAverageMR(), args);
        } catch (Exception e) {
            e.printStackTrace();
        }
        System.exit(code);
    }
}

```



# Input Data

The input data is the all the stock csv files (NYSE_daily_prices*.csv) but removed the header (first line).

# Execution

* jar -cf StockPriceAverageMR.jar StockPriceAverageMR*.class CountAverageTuple*.class
* scp StockPriceAverageMR.jar ubuntu@ip-172-31-25-109.us-west-2.compute.internal:~/downloads/
* hadoop jar StockPriceAverageMR.jar StockPriceAverageMR nyse/input nyse/average-price-combiner

Here is the running status from the console.

```bash
2018-04-03 10:01:16,818 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
2018-04-03 10:01:17,260 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1522770916485_0004
2018-04-03 10:01:17,443 INFO input.FileInputFormat: Total input files to process : 1
2018-04-03 10:01:17,495 INFO mapreduce.JobSubmitter: number of splits:4
2018-04-03 10:01:17,529 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
2018-04-03 10:01:17,644 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1522770916485_0004
2018-04-03 10:01:17,645 INFO mapreduce.JobSubmitter: Executing with tokens: []
2018-04-03 10:01:17,824 INFO conf.Configuration: resource-types.xml not found
2018-04-03 10:01:17,825 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2018-04-03 10:01:17,884 INFO impl.YarnClientImpl: Submitted application application_1522770916485_0004
2018-04-03 10:01:17,917 INFO mapreduce.Job: The url to track the job: http://ip-172-31-25-109.us-west-2.compute.internal:8088/proxy/application_1522770916485_0004/
2018-04-03 10:01:17,917 INFO mapreduce.Job: Running job: job_1522770916485_0004
2018-04-03 10:01:25,009 INFO mapreduce.Job: Job job_1522770916485_0004 running in uber mode : false
2018-04-03 10:01:25,011 INFO mapreduce.Job:  map 0% reduce 0%
2018-04-03 10:01:44,155 INFO mapreduce.Job:  map 50% reduce 0%
2018-04-03 10:01:45,173 INFO mapreduce.Job:  map 58% reduce 0%
2018-04-03 10:01:46,179 INFO mapreduce.Job:  map 75% reduce 0%
2018-04-03 10:01:56,239 INFO mapreduce.Job:  map 100% reduce 0%
2018-04-03 10:01:58,246 INFO mapreduce.Job:  map 100% reduce 100%
2018-04-03 10:01:58,251 INFO mapreduce.Job: Job job_1522770916485_0004 completed successfully
2018-04-03 10:01:58,333 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=2526
		FILE: Number of bytes written=1032690
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=511095051
		HDFS: Number of bytes written=1018
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Killed map tasks=1
		Launched map tasks=4
		Launched reduce tasks=1
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=131514
		Total time spent by all reduces in occupied slots (ms)=19338
		Total time spent by all map tasks (ms)=65757
		Total time spent by all reduce tasks (ms)=9669
		Total vcore-milliseconds taken by all map tasks=65757
		Total vcore-milliseconds taken by all reduce tasks=9669
		Total megabyte-milliseconds taken by all map tasks=134670336
		Total megabyte-milliseconds taken by all reduce tasks=19802112
	Map-Reduce Framework
		Map input records=9211031
		Map output records=9211031
		Map output bytes=110532372
		Map output materialized bytes=2544
		Input split bytes=516
		Combine input records=9211031
		Combine output records=180
		Reduce input groups=49
		Reduce shuffle bytes=2544
		Reduce input records=180
		Reduce output records=49
		Spilled Records=360
		Shuffled Maps =4
		Failed Shuffles=0
		Merged Map outputs=4
		GC time elapsed (ms)=1060
		CPU time spent (ms)=33610
		Physical memory (bytes) snapshot=2452738048
		Virtual memory (bytes) snapshot=15479181312
		Total committed heap usage (bytes)=2000683008
		Peak Map Physical memory (bytes)=607711232
		Peak Map Virtual memory (bytes)=3101917184
		Peak Reduce Physical memory (bytes)=228593664
		Peak Reduce Virtual memory (bytes)=3108765696
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=511094535
	File Output Format Counters 
		Bytes Written=1018
```



# Output Data

```bash
$fs -cat nyse/average-price-combiner/part-r-00000
1962	2520	0.50793654
1963	2509	0.5707453
1964	2530	0.7320158
1965	2520	0.8630952
1966	2519	1.0206431
1967	2510	1.3099601
1968	2260	1.3995575
1969	2500	1.4412
1970	7366	4.426419
1971	7337	5.7530327
1972	7365	7.5747457
1973	7560	7.590476
1974	7590	6.2770753
1975	7590	7.6837945
1976	7590	9.96917
1977	11529	11.32917
1978	12096	11.552331
1979	12144	12.784585
1980	14712	9.1217375
1981	14975	5.7117195
1982	26835	3.155245
1983	33893	4.682796
1984	44326	4.3459597
1985	67063	4.4323845
1986	76588	5.764898
1987	89480	6.719144
1988	138926	5.5226884
1989	149917	6.1024566
1990	183926	5.8779726
1991	199344	6.667188
1992	233612	7.135939
1993	254104	8.166081
1994	272928	8.977019
1995	294827	9.803919
1996	328170	11.942091
1997	344971	15.029295
1998	364971	16.771639
1999	382829	17.009083
2000	396071	18.496153
2001	416918	18.749891
2002	449501	18.22068
2003	484726	18.876835
2004	538095	23.50713
2005	576813	26.194454
2006	609136	29.034554
2007	651150	32.927338
2008	691940	26.877226
2009	700962	21.604937
2010	71287	26.357904
```

# Notes

As I mentioned in the beginning, this combiner will lead some loss of precision (see the result below of not using a combiner). Using `double` type for average instead of float in the `CountAverageTuple.class` will help to improve the precision. However, if the precision matters very much, you shuold not use a combiner at all.

```bash
$ fs -cat nyse/average-price/part-r-00000
1962	2520	0.50793654
1963	2509	0.5707453
1964	2530	0.7320158
1965	2520	0.8630952
1966	2519	1.0206431
1967	2510	1.3099601
1968	2260	1.3995575
1969	2500	1.4416
1970	7366	4.426419
1971	7337	5.7530327
1972	7365	7.574881
1973	7560	7.590476
1974	7590	6.2770753
1975	7590	7.6837945
1976	7590	9.96917
1977	11529	11.32917
1978	12096	11.552331
1979	12144	12.784585
1980	14712	9.1217375
1981	14975	5.7117195
1982	26835	3.155245
1983	33893	4.682796
1984	44326	4.3459597
1985	67063	4.433309
1986	76588	5.7689977
1987	89480	6.727168
1988	138926	5.5353856
1989	149917	6.1172915
1990	183926	5.8920054
1991	199344	6.688925
1992	233612	7.1677356
1993	254104	8.194786
1994	272928	9.01658
1995	294827	9.856377
1996	328170	11.996209
1997	344971	15.116392
1998	364971	16.880875
1999	382829	17.11035
2000	396071	18.62303
2001	416918	18.87149
2002	449501	18.334768
2003	484726	19.028307
2004	538095	23.72387
2005	576813	26.42284
2006	609136	29.292576
2007	651150	33.172607
2008	691940	27.11646
2009	700962	21.843964
2010	71287	26.389875
```

