# Requirements

Redo Part 5 using Memory-Conscious Median and Standard Deviation implementation as explained in the Slides (MR Summarization Patterns Slides).

Use a Combiner for optimization.

# Analysis

Using a combiner to collect the number of every rating can reduce the memory and network traffic by aggregating data.

The mapper could emit the <movieId, rating> to the combiner, then the combiner aggregate is to <movieId, {1=a, 2=b, 3=c, 4=d, 5=e}>, the value of which is a string representing of a map, where a, b, c, d, e is the number of occurrences for each rating of the movie.

The reducer will then deserilized the strings as maps and then may combine several map as one map for a movie if there are multiple maps. For example: 

 <moviedX, [ {1=a, 3=c, 4=d, 5=e, 2=b}, {1=x, 4=y, 5=z}] > will become:

 <moviedX,  {1=a+x, 2=b, 3=c, 4=d+y, 5=e+z}>

Taking this merged map as input, I use the following algorithm to calculate the median.

- count the total number of ratings denoted as `counts`
- divide `counts` by 2 denoted as `num`
- counting down from the smallest key in this map until the `num` becomes `0`, and remember the value as `x`
- counting down from the biggest key in this map until the `num` becomes `0`, and remember the value as `y`
- the median will be `(x+y)/2`

Then, use the standard deviation formula to calculate this value.

# MapReduce Program

```java
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class MovieMedianStdDevMCR extends Configured implements Tool {

    public static class TheMapper extends Mapper<Object, Text, Text, Text> {
        
        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            String[] fields = value.toString().split("::");

            if (fields == null || fields.length < 2) return;

            Text movieID = new Text(fields[1]);
            Text rating = new Text(fields[2]);
            
            context.write(movieID, rating);
        }
    }

    public static class TheCombiner extends Reducer<Text, Text, Text, Text> {
        
        Map<String, Integer> map = new HashMap<>();
        
        protected void reduce(Text key, Iterable<Text> values, Context context)
                throws IOException, InterruptedException {
            
            map.clear();
            
            for(Text v: values) {
                String rating = v.toString();
                map.merge(rating, 1, (exist, addon)->exist.intValue() + addon.intValue());
            }
            
            //value format: {3=1, 4=2, 5=1}
            context.write(key, new Text(map.toString()));
            
        }
    }

    public static class TheReducer extends Reducer<Text, Text, Text, MedianStdDevTuple> {
        
        MedianStdDevTuple result = new MedianStdDevTuple();
        Map<Integer, Integer> map = new HashMap<>();

        @Override
        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {

            map.clear();
            
//          {3=1, 4=2, 5=1}
            for(Text v: values) {
                String flatMap = v.toString().substring(1, v.getLength()-1); // remove {}
                String[] entries = flatMap.split(", ");
                
                if(entries == null || entries.length == 0) return; // skipped the empty ones
                
                for(String e: entries) { // 3=1, 4=2, 5=1
                    String[] keyValue = e.split("=");
                    int rating = Integer.valueOf(keyValue[0]);
                    int num = Integer.valueOf(keyValue[1]);
                    map.merge(rating, num, (exist, addon) -> exist.intValue() + addon.intValue());
                }
            }
            
            result.setMedian(getMedian(map));
            result.setStdDev((float)getStdDev(map));
            
            context.write(key, result);
        }
    }

    public static Float getMedian(Map<Integer, Integer> map) {
        long count = map.values().stream().mapToInt(Integer::valueOf).sum();
        
        Integer lowMedian = getNth(map, (count+1)/2);
        Integer highMedian = getNth(map, -(count+1)/2);
        
        if (lowMedian !=null && highMedian != null) {
            return ( lowMedian + highMedian ) / 2.0f;
        }
        else return null;
    }
    
    /**
     * Find the k-th element (start from 1) in the frequency map <key, occurrence>
     * @param map: frequency map <key, occurrence>
     * @param k: forward if k > 0, backward if k < 0, the smallest key if k = 0
     * @return the k-th key in the map
     */
    public static Integer getNth(Map<Integer, Integer> map, long n) {
        
        List<Integer> keys = new ArrayList<Integer>(map.keySet());
        Collections.sort(keys);
        
        if( n<0 ) {
            Collections.reverse(keys); // backward
            n =-n;
        }
        
        for(Integer key: keys) {
            n -= map.get(key);
            if (n <= 0) {
                return key;
            }
        }
        
        return null;
    }
    
    public static  double getAverage(Map<Integer, Integer> map) {
        long count = map.values().stream().mapToInt(Integer::valueOf).sum();
        long sum = map.entrySet().stream().mapToInt((e) -> e.getKey() * e.getValue()).sum();
        return 1.0 * sum/count;
    }

    public static  double getStdDev(Map<Integer, Integer> map) {
        if(map.size() < 2) return 0;
        
        double average = getAverage(map);

        double sum = map.entrySet().stream().mapToDouble( e -> ( Math.pow(e.getKey() - average, 2)) * e.getValue()).sum();
        long n = map.values().stream().mapToInt(Integer::valueOf).sum();

        double stddev = 0;
        if (n > 1) {
            stddev = Math.sqrt(sum/(n-1));
        }
        
        return stddev;
    }

    @Override
    public int run(String[] args) throws Exception {
        Configuration conf = this.getConf();

        Job job = Job.getInstance(conf, "Rating Median and StdDev (Optimized Memoery)");
        job.setJarByClass(MovieMedianStdDevMCR.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        job.setInputFormatClass(TextInputFormat.class);

        job.setMapperClass(TheMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);
        job.setCombinerClass(TheCombiner.class);

        job.setReducerClass(TheReducer.class);
        job.setNumReduceTasks(1);

        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(MedianStdDevTuple.class);

        return job.waitForCompletion(true) ? 0 : 1;
    }

    public static void main(String[] args) {
        int code = -1;
        try {
            code = ToolRunner.run(new Configuration(), new MovieMedianStdDevMCR(), args);
        } catch (Exception e) {
            e.printStackTrace();
        }
        System.exit(code);
    }
}
```

Note: MedianStdDevTuple.class is the same with HW5-part 5 since it is reusable.

This program runs well in the small scale sample data, then it is run on the full data set. Here is how.

# Data Cleaning

The headlines have been removed in previous homework for this data set so I just reuse it.

# Packaging and Running

Go to the `bin` folder of Eclipse in the local machine, and then package the jar file.

```bash
$jar -cf MovieMedianStdDevMCR.jar MovieMedianStdDevMCR*.class MedianStdDevTuple.class
```

Then, transfer this file to the remote hadoop server.

```bash
$scp MovieMedianStdDevMCR.jar ubuntu@ip-172-31-25-109.us-west-2.compute.internal:~/downloads/
```

Finally, run the hadoop command on the server.

```bash
$ hadoop jar MovieMedianStdDevMCR.jar MovieMedianStdDevMCR movielens/ml-1m/rating/input movielens/ml-1m/rating/median_stddev_combiner
2018-04-08 08:21:18,460 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
2018-04-08 08:21:18,832 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1523057534388_0007
2018-04-08 08:21:19,033 INFO input.FileInputFormat: Total input files to process : 1
2018-04-08 08:21:19,504 INFO mapreduce.JobSubmitter: number of splits:1
2018-04-08 08:21:19,546 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
2018-04-08 08:21:19,671 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1523057534388_0007
2018-04-08 08:21:19,672 INFO mapreduce.JobSubmitter: Executing with tokens: []
2018-04-08 08:21:19,850 INFO conf.Configuration: resource-types.xml not found
2018-04-08 08:21:19,851 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2018-04-08 08:21:19,905 INFO impl.YarnClientImpl: Submitted application application_1523057534388_0007
2018-04-08 08:21:19,940 INFO mapreduce.Job: The url to track the job: http://ip-172-31-25-109.us-west-2.compute.internal:8088/proxy/application_1523057534388_0007/
2018-04-08 08:21:19,941 INFO mapreduce.Job: Running job: job_1523057534388_0007
2018-04-08 08:21:26,095 INFO mapreduce.Job: Job job_1523057534388_0007 running in uber mode : false
2018-04-08 08:21:26,096 INFO mapreduce.Job:  map 0% reduce 0%
2018-04-08 08:21:34,183 INFO mapreduce.Job:  map 100% reduce 0%
2018-04-08 08:21:39,211 INFO mapreduce.Job:  map 100% reduce 100%
2018-04-08 08:21:40,223 INFO mapreduce.Job: Job job_1523057534388_0007 completed successfully
2018-04-08 08:21:40,307 INFO mapreduce.Job: Counters: 53
    File System Counters
        FILE: Number of bytes read=127044
        FILE: Number of bytes written=665219
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=24594270
        HDFS: Number of bytes written=68665
        HDFS: Number of read operations=8
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=2
    Job Counters 
        Launched map tasks=1
        Launched reduce tasks=1
        Data-local map tasks=1
        Total time spent by all maps in occupied slots (ms)=10598
        Total time spent by all reduces in occupied slots (ms)=5380
        Total time spent by all map tasks (ms)=5299
        Total time spent by all reduce tasks (ms)=2690
        Total vcore-milliseconds taken by all map tasks=5299
        Total vcore-milliseconds taken by all reduce tasks=2690
        Total megabyte-milliseconds taken by all map tasks=10852352
        Total megabyte-milliseconds taken by all reduce tasks=5509120
    Map-Reduce Framework
        Map input records=1000209
        Map output records=1000209
        Map output bytes=6721203
        Map output materialized bytes=127044
        Input split bytes=139
        Combine input records=1000209
        Combine output records=3706
        Reduce input groups=3706
        Reduce shuffle bytes=127044
        Reduce input records=3706
        Reduce output records=3706
        Spilled Records=7412
        Shuffled Maps =1
        Failed Shuffles=0
        Merged Map outputs=1
        GC time elapsed (ms)=169
        CPU time spent (ms)=5810
        Physical memory (bytes) snapshot=744280064
        Virtual memory (bytes) snapshot=6206099456
        Total committed heap usage (bytes)=587726848
        Peak Map Physical memory (bytes)=515915776
        Peak Map Virtual memory (bytes)=3097591808
        Peak Reduce Physical memory (bytes)=228364288
        Peak Reduce Virtual memory (bytes)=3108507648
    Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
    File Input Format Counters 
        Bytes Read=24594131
    File Output Format Counters 
        Bytes Written=68665
```

# Output

The output is attached as HW5-6.output.txt. Here are the first and last 10 lines for a quick look.

```reStructuredText
1    4.0    0.8523491
10    4.0    0.89123285
100    3.0    0.96187156
1000    3.0    1.2343761
1002    4.5    0.8864053
1003    3.0    0.8688836
1004    3.0    1.1685652
1005    2.0    1.114757
1006    3.0    0.9559913
1007    3.0    0.93255746
...
99    4.0    0.9733175
990    3.0    0.94373506
991    4.0    0.86484903
992    3.0    1.0954452
993    3.0    1.0540925
994    4.0    0.8186184
996    3.0    1.1267687
997    3.0    0.9893614
998    3.0    1.0580399
999    3.0    0.92159176
```

# Validation

The result matches part 5 so it is validated.

# Observation

By comparing the running summary of the MapReduce job in part 5 and part 6. I find the peak reduce memory reduced 17,084,416 bytes from 245448704 to 228364288 bytes. Also, the `Reduce input records` was cut to 0.37% from 1000209 to 3706 with the help of combiner.