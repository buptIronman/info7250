# Requirements

Using the MoviLens dataset, determine the median and standard deviation of ratings per movie.
Iterate through the given set of values and add each value to an in-memory list. The iteration also calculates a running sum and count.

# Input Data

Input data will be ml-1m/rating.dat in the following format:

UserID::MovieID::Rating::Timestamp

\- UserIDs range between 1 and 6040 

\- MovieIDs range between 1 and 3952

\- Ratings are made on a 5-star scale (whole-star ratings only)

\- Timestamp is represented in seconds since the epoch as returned by time(2)

\- Each user has at least 20 ratings

So, the key will be 'MovieID', and the value will be 'Rating'. However, in order to store two values median and stddev in the same time, I need to define a writable object as below.

# MedianStdDevTuple

```java
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

import org.apache.hadoop.io.Writable;

public class MedianStdDevTuple implements Writable {
	private float median;
	private float stdDev;

	public float getMedian() {
		return median;
	}

	public void setMedian(float median) {
		this.median = median;
	}

	public float getStdDev() {
		return stdDev;
	}

	public void setStdDev(float stdDev) {
		this.stdDev = stdDev;
	}

	@Override
	public void readFields(DataInput arg0) throws IOException {
		median = arg0.readFloat();
		stdDev = arg0.readFloat();
	}

	@Override
	public void write(DataOutput arg0) throws IOException {
		arg0.writeFloat(median);
		arg0.writeFloat(stdDev);
	}

	@Override
	public String toString() {
		return median + "\t" + stdDev;
	}

	@Override
	public int hashCode() {
		final int prime = 31;
		int result = 1;
		result = prime * result + Float.floatToIntBits(median);
		result = prime * result + Float.floatToIntBits(stdDev);
		return result;
	}

	@Override
	public boolean equals(Object obj) {
		if (this == obj)
			return true;
		if (obj == null)
			return false;
		if (getClass() != obj.getClass())
			return false;
		MedianStdDevTuple other = (MedianStdDevTuple) obj;
		if (Float.floatToIntBits(median) != Float.floatToIntBits(other.median))
			return false;
		if (Float.floatToIntBits(stdDev) != Float.floatToIntBits(other.stdDev))
			return false;
		return true;
	}

}
```

# MapReduce

## Map

```java
public static class TheMapper extends Mapper<Object, Text, Text, IntWritable> {

    private Text movieID = new Text(); // Text is more generic than IntWritable
    private IntWritable rating = new IntWritable(); // whole-star ratings only

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        String[] fields = value.toString().split("::");
        movieID.set(fields[1]);
        rating.set(Integer.parseInt(fields[2]));
        context.write(movieID, rating);
    }
}  
```



## Reduce

```java
public static class TheReducer extends Reducer<Text, IntWritable, Text, MedianStdDevTuple> {

    private MedianStdDevTuple result = new MedianStdDevTuple();
    private List<Integer> ratings = new ArrayList<Integer>();

    @Override
    public void reduce(Text key, Iterable<IntWritable> values, Context context)
        throws IOException, InterruptedException {

        for(IntWritable v: values) {
            ratings.add(v.get());
        }

        result.setMedian((float)getMedian(ratings));
        result.setStdDev((float)getStdDev(ratings));

        context.write(key, result);
    }

    private double getMedian(List<Integer> values) {
        List<Integer> list = new ArrayList<Integer>(values);
        Collections.sort(list);

        int size = list.size();
        if (list.size()%2 == 0) {
            return (list.get(size/2) + list.get((size-1)/2)) / 2.0; 
        } else {
            return list.get(size/2);
        }
    }

    private double getStdDev(List<Integer> values) {
        double average = values.stream().mapToInt(i->i.intValue()).summaryStatistics().getAverage();

        double sum = 0;
        for(Integer v: values) {
            sum += Math.pow(v.intValue() - average, 2);
        }

        double stddev = 0;
        if(values.size() > 1) {
            stddev = sum / (values.size() - 1);
        }
        stddev = Math.sqrt(stddev);

        return stddev;
    }
}
```

## Driver

```java
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class MovieMedianStdDevMR extends Configured implements Tool {
    
    @Override
    public int run(String[] args) throws Exception {
        Configuration conf = this.getConf();

        Job job = Job.getInstance(conf, "Rating Median and StdDev");
        job.setJarByClass(MovieMedianStdDevMR.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        job.setInputFormatClass(TextInputFormat.class);

        job.setMapperClass(TheMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);
        
        job.setReducerClass(TheReducer.class);
        job.setNumReduceTasks(1);
        
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(MedianStdDevTuple.class);

        return job.waitForCompletion(true) ? 0 : 1;
    }

    public static void main(String[] args) {
        int code = -1;
        try {
            code = ToolRunner.run(new Configuration(), new MovieMedianStdDevMR(), args);
        } catch (Exception e) {
            e.printStackTrace();
        }
        System.exit(code);
    }
}
```



# Execution

Go to the 'bin' folder of this project in Eclipse, and the package:

```bash
jar -cf MovieMedianStdDevMR.jar MovieMedianStdDevMR*.class MedianStdDevTuple.class
```

Then transfer the Jar file to remote hadoop server:

```bash
scp StockPriceAverageMR.jar ubuntu@ip-172-31-25-109.us-west-2.compute.internal:~/downloads/
```

At last, run hadoop job:

```bash
$ hadoop jar MovieMedianStdDevMR.jar MovieMedianStdDevMR movielens/ml-1m/rating/input movielens/ml-1m/rating/average
2018-04-03 11:53:04,310 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
2018-04-03 11:53:04,729 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1522770916485_0005
2018-04-03 11:53:04,937 INFO input.FileInputFormat: Total input files to process : 1
2018-04-03 11:53:04,994 INFO mapreduce.JobSubmitter: number of splits:1
2018-04-03 11:53:05,029 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
2018-04-03 11:53:05,133 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1522770916485_0005
2018-04-03 11:53:05,134 INFO mapreduce.JobSubmitter: Executing with tokens: []
2018-04-03 11:53:05,304 INFO conf.Configuration: resource-types.xml not found
2018-04-03 11:53:05,305 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2018-04-03 11:53:05,363 INFO impl.YarnClientImpl: Submitted application application_1522770916485_0005
2018-04-03 11:53:05,406 INFO mapreduce.Job: The url to track the job: http://ip-172-31-25-109.us-west-2.compute.internal:8088/proxy/application_1522770916485_0005/
2018-04-03 11:53:05,408 INFO mapreduce.Job: Running job: job_1522770916485_0005
2018-04-03 11:53:11,505 INFO mapreduce.Job: Job job_1522770916485_0005 running in uber mode : false
2018-04-03 11:53:11,507 INFO mapreduce.Job:  map 0% reduce 0%
2018-04-03 11:53:18,571 INFO mapreduce.Job:  map 100% reduce 0%
2018-04-03 11:53:35,668 INFO mapreduce.Job:  map 100% reduce 76%
2018-04-03 11:53:41,704 INFO mapreduce.Job:  map 100% reduce 79%
2018-04-03 11:53:47,734 INFO mapreduce.Job:  map 100% reduce 80%
2018-04-03 11:53:53,764 INFO mapreduce.Job:  map 100% reduce 82%
2018-04-03 11:53:59,802 INFO mapreduce.Job:  map 100% reduce 83%
2018-04-03 11:54:05,838 INFO mapreduce.Job:  map 100% reduce 85%
2018-04-03 11:54:11,870 INFO mapreduce.Job:  map 100% reduce 86%
2018-04-03 11:54:17,896 INFO mapreduce.Job:  map 100% reduce 87%
2018-04-03 11:54:23,921 INFO mapreduce.Job:  map 100% reduce 88%
2018-04-03 11:54:29,954 INFO mapreduce.Job:  map 100% reduce 89%
2018-04-03 11:54:35,981 INFO mapreduce.Job:  map 100% reduce 90%
2018-04-03 11:54:42,009 INFO mapreduce.Job:  map 100% reduce 91%
2018-04-03 11:54:53,056 INFO mapreduce.Job:  map 100% reduce 92%
2018-04-03 11:54:59,082 INFO mapreduce.Job:  map 100% reduce 93%
2018-04-03 11:55:05,110 INFO mapreduce.Job:  map 100% reduce 94%
2018-04-03 11:55:11,135 INFO mapreduce.Job:  map 100% reduce 95%
2018-04-03 11:55:29,214 INFO mapreduce.Job:  map 100% reduce 96%
2018-04-03 11:55:35,235 INFO mapreduce.Job:  map 100% reduce 97%
2018-04-03 11:55:41,253 INFO mapreduce.Job:  map 100% reduce 98%
2018-04-03 11:55:59,329 INFO mapreduce.Job:  map 100% reduce 99%
2018-04-03 11:56:05,351 INFO mapreduce.Job:  map 100% reduce 100%
2018-04-03 11:56:12,375 INFO mapreduce.Job: Job job_1522770916485_0005 completed successfully
2018-04-03 11:56:12,453 INFO mapreduce.Job: Counters: 53
	File System Counters
		FILE: Number of bytes read=10722045
		FILE: Number of bytes written=21854781
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=24594270
		HDFS: Number of bytes written=68876
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=9800
		Total time spent by all reduces in occupied slots (ms)=341388
		Total time spent by all map tasks (ms)=4900
		Total time spent by all reduce tasks (ms)=170694
		Total vcore-milliseconds taken by all map tasks=4900
		Total vcore-milliseconds taken by all reduce tasks=170694
		Total megabyte-milliseconds taken by all map tasks=10035200
		Total megabyte-milliseconds taken by all reduce tasks=349581312
	Map-Reduce Framework
		Map input records=1000209
		Map output records=1000209
		Map output bytes=8721621
		Map output materialized bytes=10722045
		Input split bytes=139
		Combine input records=0
		Combine output records=0
		Reduce input groups=3706
		Reduce shuffle bytes=10722045
		Reduce input records=1000209
		Reduce output records=3706
		Spilled Records=2000418
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=877
		CPU time spent (ms)=174370
		Physical memory (bytes) snapshot=678825984
		Virtual memory (bytes) snapshot=6179840000
		Total committed heap usage (bytes)=472907776
		Peak Map Physical memory (bytes)=476811264
		Peak Map Virtual memory (bytes)=3098451968
		Peak Reduce Physical memory (bytes)=401158144
		Peak Reduce Virtual memory (bytes)=3081388032
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=24594131
	File Output Format Counters 
		Bytes Written=68876
```



# Output Data

The whole output is attached as 'HW5-5.output.txt', here is sample output (first 20 lines and last 20 lines):

<pre>
1	4.0	0.8523491
10	4.0	0.9075746
100	4.0	0.92732733
1000	4.0	0.93200505
1002	4.0	0.9319033
1003	4.0	0.9479233
1004	4.0	0.97780836
1005	4.0	1.0259838
1006	4.0	1.0294818
1007	4.0	1.0413051
1008	4.0	1.0406955
1009	4.0	1.0490336
101	4.0	1.0428864
1010	4.0	1.0438097
1011	4.0	1.054326
1012	4.0	1.0488594
1013	4.0	1.0480685
1014	4.0	1.0509071
1015	4.0	1.0525066
1016	4.0	1.0514407
...
98	4.0	1.1171877
980	4.0	1.1171894
981	4.0	1.1171924
982	4.0	1.1171795
984	4.0	1.1171911
985	4.0	1.117191
986	4.0	1.1171685
987	4.0	1.1171714
988	4.0	1.1171683
989	4.0	1.1171687
99	4.0	1.1171618
990	4.0	1.1171772
991	4.0	1.1171391
992	4.0	1.1171468
993	4.0	1.1171466
994	4.0	1.1170831
996	4.0	1.1171373
997	4.0	1.1171341
998	4.0	1.1171418
999	4.0	1.1171018
</pre>