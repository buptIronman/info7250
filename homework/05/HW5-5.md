# Requirements

Using the MoviLens dataset, determine the median and standard deviation of ratings per movie.
Iterate through the given set of values and add each value to an in-memory list. The iteration also calculates a running sum and count.

# Input Data

Input data will be ml-1m/rating.dat in the following format:

UserID::MovieID::Rating::Timestamp

\- UserIDs range between 1 and 6040 

\- MovieIDs range between 1 and 3952

\- Ratings are made on a 5-star scale (whole-star ratings only)

\- Timestamp is represented in seconds since the epoch as returned by time(2)

\- Each user has at least 20 ratings

So, the key will be 'MovieID', and the value will be 'Rating'. However, in order to store two values median and stddev in the same time, I need to define a writable object as below.

# MedianStdDevTuple

```java
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

import org.apache.hadoop.io.Writable;

public class MedianStdDevTuple implements Writable {
    private float median;
    private float stdDev;

    public float getMedian() {
        return median;
    }

    public void setMedian(float median) {
        this.median = median;
    }

    public float getStdDev() {
        return stdDev;
    }

    public void setStdDev(float stdDev) {
        this.stdDev = stdDev;
    }

    @Override
    public void readFields(DataInput arg0) throws IOException {
        median = arg0.readFloat();
        stdDev = arg0.readFloat();
    }

    @Override
    public void write(DataOutput arg0) throws IOException {
        arg0.writeFloat(median);
        arg0.writeFloat(stdDev);
    }

    @Override
    public String toString() {
        return median + "\t" + stdDev;
    }

    @Override
    public int hashCode() {
        final int prime = 31;
        int result = 1;
        result = prime * result + Float.floatToIntBits(median);
        result = prime * result + Float.floatToIntBits(stdDev);
        return result;
    }

    @Override
    public boolean equals(Object obj) {
        if (this == obj)
            return true;
        if (obj == null)
            return false;
        if (getClass() != obj.getClass())
            return false;
        MedianStdDevTuple other = (MedianStdDevTuple) obj;
        if (Float.floatToIntBits(median) != Float.floatToIntBits(other.median))
            return false;
        if (Float.floatToIntBits(stdDev) != Float.floatToIntBits(other.stdDev))
            return false;
        return true;
    }

}
```

# MapReduce

## Map

```java
public static class TheMapper extends Mapper<Object, Text, Text, IntWritable> {

    private Text movieID = new Text(); // Text is more generic than IntWritable
    private IntWritable rating = new IntWritable(); // whole-star ratings only

    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        String[] fields = value.toString().split("::");
        movieID.set(fields[1]);
        rating.set(Integer.parseInt(fields[2]));
        context.write(movieID, rating);
    }
}  
```



## Reduce

```java
public static class TheReducer extends Reducer<Text, IntWritable, Text, MedianStdDevTuple> {

    private MedianStdDevTuple result = new MedianStdDevTuple();
    private List<Integer> ratings = new ArrayList<Integer>();

    @Override
    public void reduce(Text key, Iterable<IntWritable> values, Context context)
        throws IOException, InterruptedException {

        for(IntWritable v: values) {
            ratings.add(v.get());
        }

        result.setMedian((float)getMedian(ratings));
        result.setStdDev((float)getStdDev(ratings));

        context.write(key, result);
    }

    private double getMedian(List<Integer> values) {
        List<Integer> list = new ArrayList<Integer>(values);
        Collections.sort(list);

        int size = list.size();
        if (list.size()%2 == 0) {
            return (list.get(size/2) + list.get((size-1)/2)) / 2.0; 
        } else {
            return list.get(size/2);
        }
    }

    private double getStdDev(List<Integer> values) {
        double average = values.stream().mapToInt(i->i.intValue()).summaryStatistics().getAverage();

        double sum = 0;
        for(Integer v: values) {
            sum += Math.pow(v.intValue() - average, 2);
        }

        double stddev = 0;
        if(values.size() > 1) {
            stddev = sum / (values.size() - 1);
        }
        stddev = Math.sqrt(stddev);

        return stddev;
    }
}
```

## Driver

```java
import java.io.IOException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class MovieMedianStdDevMR extends Configured implements Tool {
    
    @Override
    public int run(String[] args) throws Exception {
        Configuration conf = this.getConf();

        Job job = Job.getInstance(conf, "Rating Median and StdDev");
        job.setJarByClass(MovieMedianStdDevMR.class);

        FileInputFormat.addInputPath(job, new Path(args[0]));
        job.setInputFormatClass(TextInputFormat.class);

        job.setMapperClass(TheMapper.class);
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);
        
        job.setReducerClass(TheReducer.class);
        job.setNumReduceTasks(1);
        
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(MedianStdDevTuple.class);

        return job.waitForCompletion(true) ? 0 : 1;
    }

    public static void main(String[] args) {
        int code = -1;
        try {
            code = ToolRunner.run(new Configuration(), new MovieMedianStdDevMR(), args);
        } catch (Exception e) {
            e.printStackTrace();
        }
        System.exit(code);
    }
}
```



# Execution

Go to the 'bin' folder of this project in Eclipse, and the package:

```bash
$jar -cf MovieMedianStdDevMR.jar MovieMedianStdDevMR*.class MedianStdDevTuple.class
```

Then transfer the Jar file to remote hadoop server:

```bash
$scp MovieMedianStdDevMR.jar ubuntu@ip-172-31-25-109.us-west-2.compute.internal:~/downloads/
MovieMedianStdDevMR.jar
```

At last, run hadoop job:

```bash
$ hadoop jar MovieMedianStdDevMR.jar MovieMedianStdDevMR movielens/ml-1m/rating/input movielens/ml-1m/rating/median_stddev2018-04-08 08:26:11,141 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
2018-04-08 08:26:11,542 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1523057534388_0008
2018-04-08 08:26:11,741 INFO input.FileInputFormat: Total input files to process : 1
2018-04-08 08:26:11,798 INFO mapreduce.JobSubmitter: number of splits:1
2018-04-08 08:26:11,831 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
2018-04-08 08:26:11,943 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1523057534388_0008
2018-04-08 08:26:11,944 INFO mapreduce.JobSubmitter: Executing with tokens: []
2018-04-08 08:26:12,110 INFO conf.Configuration: resource-types.xml not found
2018-04-08 08:26:12,111 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2018-04-08 08:26:12,162 INFO impl.YarnClientImpl: Submitted application application_1523057534388_0008
2018-04-08 08:26:12,196 INFO mapreduce.Job: The url to track the job: http://ip-172-31-25-109.us-west-2.compute.internal:8088/proxy/application_1523057534388_0008/
2018-04-08 08:26:12,197 INFO mapreduce.Job: Running job: job_1523057534388_0008
2018-04-08 08:26:18,344 INFO mapreduce.Job: Job job_1523057534388_0008 running in uber mode : false
2018-04-08 08:26:18,345 INFO mapreduce.Job:  map 0% reduce 0%
2018-04-08 08:26:26,426 INFO mapreduce.Job:  map 100% reduce 0%
2018-04-08 08:26:32,459 INFO mapreduce.Job:  map 100% reduce 100%
2018-04-08 08:26:32,468 INFO mapreduce.Job: Job job_1523057534388_0008 completed successfully
2018-04-08 08:26:32,551 INFO mapreduce.Job: Counters: 53
    File System Counters
        FILE: Number of bytes read=10722045
        FILE: Number of bytes written=21854793
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=24594270
        HDFS: Number of bytes written=68665
        HDFS: Number of read operations=8
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=2
    Job Counters 
        Launched map tasks=1
        Launched reduce tasks=1
        Data-local map tasks=1
        Total time spent by all maps in occupied slots (ms)=10190
        Total time spent by all reduces in occupied slots (ms)=7130
        Total time spent by all map tasks (ms)=5095
        Total time spent by all reduce tasks (ms)=3565
        Total vcore-milliseconds taken by all map tasks=5095
        Total vcore-milliseconds taken by all reduce tasks=3565
        Total megabyte-milliseconds taken by all map tasks=10434560
        Total megabyte-milliseconds taken by all reduce tasks=7301120
    Map-Reduce Framework
        Map input records=1000209
        Map output records=1000209
        Map output bytes=8721621
        Map output materialized bytes=10722045
        Input split bytes=139
        Combine input records=0
        Combine output records=0
        Reduce input groups=3706
        Reduce shuffle bytes=10722045
        Reduce input records=1000209
        Reduce output records=3706
        Spilled Records=2000418
        Shuffled Maps =1
        Failed Shuffles=0
        Merged Map outputs=1
        GC time elapsed (ms)=164
        CPU time spent (ms)=5850
        Physical memory (bytes) snapshot=737845248
        Virtual memory (bytes) snapshot=6206992384
        Total committed heap usage (bytes)=560988160
        Peak Map Physical memory (bytes)=492396544
        Peak Map Virtual memory (bytes)=3097944064
        Peak Reduce Physical memory (bytes)=245448704
        Peak Reduce Virtual memory (bytes)=3109048320
    Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
    File Input Format Counters 
        Bytes Read=24594131
    File Output Format Counters 
        Bytes Written=68665
```



# Output Data

The whole output is attached as 'HW5-5.output.txt', here is sample output (first 10 lines and last 10 lines):

>1    4.0    0.8523491
>10    4.0    0.89123285
>100    3.0    0.96187156
>1000    3.0    1.2343761
>1002    4.5    0.8864053
>1003    3.0    0.8688836
>1004    3.0    1.1685652
>1005    2.0    1.114757
>1006    3.0    0.9559913
>1007    3.0    0.93255746
>...
>99    4.0    0.9733175
>990    3.0    0.94373506
>991    4.0    0.86484903
>992    3.0    1.0954452
>993    3.0    1.0540925
>994    4.0    0.8186184
>996    3.0    1.1267687
>997    3.0    0.9893614
>998    3.0    1.0580399
>999    3.0    0.92159176

# Validation

Movie 1007 was chosen to verify the running result. Here is some fact using Excel to get a summary of this movie, which matches the running result from hadoop `0.93255746`.

> 226 ratings with median=3 stddev=0.932557442